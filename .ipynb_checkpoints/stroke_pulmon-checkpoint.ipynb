{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef729066",
   "metadata": {},
   "source": [
    "# EDA GRUPO PULMÓN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6315b991-5211-411a-8e26-b67a6d353901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías de análisis de datos:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632ddc3f-6b0d-447e-8f04-d3947e5f49f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (1.6.2)\n",
      "Requirement already satisfied: lightgbm in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: catboost in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (1.0.6)\n",
      "Requirement already satisfied: scipy in /home/anghi/.local/lib/python3.10/site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: numpy in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from xgboost) (1.23.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: wheel in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: six in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: plotly in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from catboost) (5.10.0)\n",
      "Requirement already satisfied: matplotlib in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from catboost) (3.6.0)\n",
      "Requirement already satisfied: graphviz in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from catboost) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from pandas>=0.24.0->catboost) (2022.2.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anghi/.local/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (4.37.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/anghi/miniconda3/envs/pulmon/lib/python3.10/site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e0f923-c79a-4b25-bc62-11ad414836d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12b4307-088f-4bd1-9b45-92b534896994",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './stroke_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Buenas prácticas.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path_to_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./stroke_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m data_stroke \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/pulmon/lib/python3.10/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './stroke_dataset.csv'"
     ]
    }
   ],
   "source": [
    "# Buenas prácticas.\n",
    "path_to_data = \"./stroke_dataset.csv\"\n",
    "data_stroke = pd.read_csv(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65561db-da56-45f2-8df5-fb802f42eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de entradas y columnas del dataset.\n",
    "data_stroke.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5c3e3-6eed-4328-9c74-17bfa2ac7e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos las 5 primeras entradas de nuestro dataset\n",
    "data_stroke.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00456c55-856c-4c81-9d28-39c26c72aeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información del tipo de variables de nuestro dataset\n",
    "data_stroke.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da3db4-3054-4dc1-9724-f401520c3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el número de observaciones distintas de cada variable\n",
    "data_stroke.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización del número de observaciones distintas en la variable \"Work_type\"\n",
    "data_stroke [\"work_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bb527-89db-4c32-998b-e19bca0a62a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Buscamos datos nulos \n",
    "data_stroke.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148f016",
   "metadata": {},
   "source": [
    "### Vemos que nuestro dataset no tiene datos nulos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52474972-3dbc-42d2-8fa6-63f5948c122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clasificación de variables\n",
    "categoricas=[\"gender\", \"ever_married\",\"heart_disease\", \"hypertension\",\"work_type\",\"Residence_type\",\"smoking_status\", \"stroke\"]\n",
    "numericas=[\"age\",\"avg_glucose_level\",\"bmi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffdc87-5cdc-4623-b67e-6574d5b7acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descripción de los datos numéricos con sus métricas estadísticas más relevantes.\n",
    "\n",
    "data_stroke[numericas].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d7877-1932-4a7d-9ce4-6fc4c84147ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que no hay datos duplicados\n",
    "data_stroke.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14129852",
   "metadata": {},
   "source": [
    "## Preprocesamiento: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b286332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función con Onehot encode para poder hacer dummies.\n",
    "\n",
    "def onehot_encode(df,column):\n",
    "    df = df.copy()\n",
    "    \n",
    "    dummies = pd.get_dummies( df [column],prefix=column)\n",
    "    df = pd.concat([df,dummies], axis=1)\n",
    "    df = df.drop(column,axis=1)\n",
    "    \n",
    "    return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos función que hace una copia como primer paso\n",
    "\n",
    "def preprocess_inputs(df):\n",
    "    df = df.copy()\n",
    "\n",
    "#2.- Si quicieramos eliminar la columna Id, pero en este dato no existe.\n",
    "    # df = df.drop(\"id\", axis=1)\n",
    "    \n",
    "#3.- Despues de identificar las clases  dentro de cada variable y transformamos los datos en binarios\n",
    "    df[\"ever_married\"] = df[\"ever_married\"].replace({\"No\":0,\"Yes\":1})\n",
    "    df[\"gender\"] = df[\"gender\"].replace({\"Male\":0,\"Female\":1})\n",
    "    df[\"Residence_type\"] = df[\"Residence_type\"].replace({\"Urban\":0,\"Rural\":1})\n",
    "    \n",
    "#4.-Haciendo Onehot_encode con las variables categóricas\n",
    "    for column in [\"work_type\",\"smoking_status\"]:\n",
    "        df = onehot_encode(df,column = column)\n",
    "        \n",
    "        \n",
    "#5.- Separamos la columna stroke (variable a predecir) \n",
    "    y = df['stroke']\n",
    "    X = df.drop('stroke', axis=1)\n",
    "    \n",
    "#6.- Definiendo Train-test split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n",
    "    \n",
    "#7.- Escalamos las variables Xtest, Xtrain  \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "    \n",
    "   \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d582fb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definimos la variable preprocess_inputs \n",
    "X_train, X_test, y_train, y_test = preprocess_inputs(data_stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1911d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veridficamos el funcionamiento de preprocess_inputs \n",
    "y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4158f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la varianza para comprobar la escala en X_train\n",
    "X_train.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8dc85-31bb-4568-a7e6-f061f826b368",
   "metadata": {},
   "source": [
    "## ENTRENAMOS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28829f-f333-4a7b-a545-3576a4a4cfa2",
   "metadata": {},
   "source": [
    "### Primer entrenamiento sin balanceo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef283273-fdb9-47a8-8496-4f2f4ef48f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos elegidos para nuestro entrenamiesto de clasificación \n",
    "models = {\n",
    "    \"                   Logistic Regression\": LogisticRegression(),\n",
    "    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"                         Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n",
    "    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n",
    "    \"                        Neural Network\": MLPClassifier(),\n",
    "    \"                         Random Forest\": RandomForestClassifier(),\n",
    "    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"                               XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n",
    "    \"                              LightGBM\": LGBMClassifier(),\n",
    "    \"                              CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "# entrenamiento:\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(name + \" trained.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6dd5d-9943-464d-ad6a-a453cedf6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de datos que tienen casos de stroke afirmativos y negativos donde 0=No y 1=Si\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68767fc7",
   "metadata": {},
   "source": [
    "### Nos damos cuenta que el dataset esta muy desbalanceado puesto que hay muchos casos negativos y esto puede influir sesgando los datos , pero lo solucionaremos luego\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4e76f-9e8a-4edd-83e3-62281b018eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacamos las métricas en test y train \n",
    "print(\"Model Performance\\n-----------------\")\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    print(\n",
    "        \"\\n\" + name + \" Accuracy: {:.3f}%\\n\\t\\t\\t\\t       F1-Score: {:.5f}\\n\\t\\t\\t\\t              Recall: {:.5f}\\n\\t\\t\\t\\t       Precision: {:.5f}\\n\\t\\t\\t\\t       Roc_Au: {:.5f}\\n\\t\\t\\t\\t\"\\\n",
    "        .format(accuracy_score(y_test, y_pred) * 100, f1_score(y_test, y_pred),recall_score(y_test, y_pred),precision_score(y_test, y_pred),roc_auc_score(y_test, y_pred) ))\n",
    "    \n",
    "    print( confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(\n",
    "        \"\\n\" + name + \" Accuracy_train: {:.3f}%\\n\\t\\t\\t\\t       F1-Score_train: {:.5f}\\n\\t\\t\\t\\t              Recall_Train: {:.5f}\\n\\t\\t\\t\\t       Precision_Train: {:.5f}\\n\\t\\t\\t\\t       Roc_Au_Train: {:.5f}\\n\\t\\t\\t\\t\"\\\n",
    "        .format(accuracy_score(y_train, y_pred_train) * 100, f1_score(y_train, y_pred_train),recall_score(y_train, y_pred_train),precision_score(y_train, y_pred_train),roc_auc_score(y_train, y_pred_train) ))\n",
    "\n",
    "# Imprimiendo matriz de confusión \n",
    "    print(confusion_matrix(y_train, y_pred_train))\n",
    "    \n",
    "# Medimos el overfitting\n",
    "    print(\n",
    "    np.abs(((((accuracy_score(y_train, y_pred_train))-accuracy_score(y_test, y_pred))/(accuracy_score(y_test, y_pred)) *100)))\n",
    "       )\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8298fe-2ddd-4ef4-a54b-9f84a9ee9e10",
   "metadata": {},
   "source": [
    "### Primer intento de balanceo de datos con técnica de Oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0018fb-964b-4e3d-8c0e-fdec65fc195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobamos en numero de casos positivos y negativos en y_train\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71434f41-fb10-4a7a-bc61-59f108375010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el oversample en los datos reservados para train y hacemos una copia para mayor seguridad\n",
    "oversampled_data = pd.concat([X_train, y_train], axis=1).copy()\n",
    "\n",
    "# igualamos la cantidad de datos \n",
    "num_samples = y_train.value_counts()[0] - y_train.value_counts()[1]\n",
    "new_samples = oversampled_data.query(\"stroke == 1\").sample(num_samples, replace=True, random_state=1)\n",
    "\n",
    "# definimos para oversamplear\n",
    "oversampled_data = pd.concat([oversampled_data, new_samples], axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Definimos X e y con la data banceada \n",
    "y_oversampled = oversampled_data['stroke']\n",
    "X_oversampled = oversampled_data.drop('stroke', axis=1)\n",
    "\n",
    "#Def Train-Test-Split\n",
    "X_oversampled_train, X_oversampled_test, y_oversampled_train, y_oversampled_test = train_test_split(X_oversampled, y_oversampled, train_size=0.7, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddf7ae-4721-492c-9031-e6c95e9d9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que se ha hecho bien el split\n",
    "y_oversampled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d536996-627f-47dd-a6e0-048cdc488129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización de la data oversampleada\n",
    "oversampled_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903d322-6624-4c9d-9249-aa8225d4daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobamos el balanceo de la data\n",
    "y_oversampled_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a83d3d-b451-40f1-8700-bdc03f055115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando los modelos con la data balanceada\n",
    "models = {\n",
    "    \"                   Logistic Regression\": LogisticRegression(),\n",
    "    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"                         Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n",
    "    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n",
    "    \"                        Neural Network\": MLPClassifier(),\n",
    "    \"                         Random Forest\": RandomForestClassifier(),\n",
    "    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"                               XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n",
    "    \"                              LightGBM\": LGBMClassifier(),\n",
    "    \"                              CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_oversampled_train, y_oversampled_train)\n",
    "    print(name + \" Entrenado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f4c67-e65f-4ebf-a2a8-2c8331c8ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacamos las métricas más importantes ya con los datos de Oversampled en X e y .\n",
    "\n",
    "print(\"Model Performance\\n-----------------\")\n",
    "for name, model in models.items():\n",
    "    y_oversampled_pred = model.predict(X_oversampled_test)\n",
    "    \n",
    "    y_oversampled_pred_train = model.predict(X_oversampled_train)\n",
    "\n",
    "\n",
    "    print(\n",
    "        \"\\n\" + name + \" Accuracy: {:.3f}%\\n\\t\\t\\t\\t       F1-Score: {:.5f}\\n\\t\\t\\t              Recall: {:.5f}\\n\\t\\t\\t\\t       Precision: {:.5f}\\n\\t\\t\\t\\t       Roc_Au: {:.5f}\\n\\t\\t\\t\\t\"\\\n",
    "        .format(accuracy_score(y_oversampled_test, y_oversampled_pred) * 100, f1_score(y_oversampled_test, y_oversampled_pred),recall_score(y_oversampled_test, y_oversampled_pred),precision_score(y_oversampled_test, y_oversampled_pred),roc_auc_score(y_oversampled_test, y_oversampled_pred) ))\n",
    "    \n",
    "    print( confusion_matrix(y_oversampled_test, y_oversampled_pred))\n",
    "    \n",
    "    print(\n",
    "        \"\\n\" + name + \" Accuracy_train: {:.3f}%\\n\\t\\t\\t\\t       F1-Score_train: {:.5f}\\n\\t\\t\\t\\t        Recall_Train: {:.5f}\\n\\t\\t\\t\\t       Precision_Train: {:.5f}\\n\\t\\t\\t\\t       Roc_Au_Train: {:.5f}\\n\\t\\t\\t\\t\"\\\n",
    "        .format(accuracy_score(y_oversampled_train, y_oversampled_pred_train) * 100, f1_score(y_oversampled_train, y_oversampled_pred_train),recall_score(y_oversampled_train, y_oversampled_pred_train),precision_score(y_oversampled_train, y_oversampled_pred_train),roc_auc_score(y_oversampled_train, y_oversampled_pred_train) ))\n",
    " \n",
    "    print( confusion_matrix(y_oversampled_train, y_oversampled_pred_train))\n",
    "    \n",
    "    print(\n",
    "    \"\\n\" + name + \" Overfitting:\\n\\t\\t\\t\\t\",\n",
    "    np.abs(((((accuracy_score(y_oversampled_train, y_oversampled_pred_train))-accuracy_score(y_oversampled_test, y_oversampled_pred))/(accuracy_score(y_oversampled_test, y_oversampled_pred)) *100)))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a841a9",
   "metadata": {},
   "source": [
    "### Al haber usado el metodo de oversampling para balancear , no podemos confiar en que haya duplicado datos y las predicciones no esten sesgadas. \n",
    "### Para elegir un modelo , vamos a probar otro método de balanceo y comparamos las métricas ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e76a6",
   "metadata": {},
   "source": [
    "## Segunda técnica de balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee644d26-fc6f-42b8-be3c-ed0ef9fa5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo intento de balanceo de datos con técnica de SmoteTomek\n",
    "\n",
    "data_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7279a1f-df75-4f88-8442-4d1160879011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui sacamos las X y y para procesarlas fuera de la función\n",
    "#Eliminamos Transported de X porque es el valor a predecir. La X es mayúscula\n",
    "y = data_stroke['stroke']\n",
    "X = data_stroke.drop('stroke', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef2160-3258-4863-87c6-90be79b7680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos que se ha eliminado la variable Stroke del eje X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef733ec3-78cd-498d-a4cd-ca3194aefcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que Y es la variable del stroke.\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8f9ad-d499-44af-bc12-5931953c6a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificación de las variables\n",
    "categoricas=[\"gender\", \"ever_married\",\"heart_disease\", \"hypertension\",\"work_type\",\"Residence_type\",\"smoking_status\"]\n",
    "numericas=[\"age\",\"avg_glucose_level\",\"bmi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7bde78-eec2-4e64-bdb0-d32c0398f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importanción de Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff0e77-e198-482d-9df7-15c90763243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos transformers para cada categoría de variables.\n",
    "transformer_numerico = (\"transformer_numerica\", MinMaxScaler(), numericas)\n",
    "transformer_categorico = (\"transformer_categorica\", OneHotEncoder(), categoricas)\n",
    "transformer = ColumnTransformer([transformer_numerico, transformer_categorico], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73a3f2-5aa2-4a91-9bca-006c77b6c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denominamos la X para transformer\n",
    "X = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60852dc3-1c8f-4876-8f40-fdf695d04988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un DataFrame con los transformers.\n",
    "transformer_final= pd.DataFrame(X, columns = transformer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bf9be-379e-4ec6-948e-79874a33cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el DataFrame con transformers\n",
    "transformer_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3d783-ea26-4be3-a102-a0a44591725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las variables numéricas y categóricas.\n",
    "transformer.output_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408a641-8466-443b-be45-8dcc60f51875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargando el archivo de transformer a pickle para subirlo a streamlit\n",
    "\n",
    "file_trans = open('transformer_f.pkl', 'wb')\n",
    "pickle.dump(transformer, file_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabccd5-2f97-4554-b533-62dd3f15b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumamos la variable Y en positivos para después tratarlo.\n",
    "positivo = y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a85204-8ff7-40bb-95ff-bacd1d91fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para sacar los valores negativos hemos de restar los positivos.\n",
    "negativo = y.shape[0]-positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793181c-ed5a-417d-87d3-5934960ac7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los valores positivos y negativos.\n",
    "print(\"Rpta Positivos: \", positivo, \"Rpta Negativos:\", negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c94068-24ba-45fd-b735-036b9f7018d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de imblearn para utilizar el método de SmoteTomek (no desde terminal)\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a234b-1af8-4af6-a611-eca5c62af802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de SmomteTomek\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smoteT = SMOTETomek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eeeaad-afa0-431e-bd09-9cc1c105ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiendo las variables para balancear los datos\n",
    "\n",
    "X_smoteT, y_smoteT = smoteT.fit_resample(X, y)\n",
    "Positivo_smoteT = y_smoteT.sum()\n",
    "Negativo_smoteT = y_smoteT.shape[0]-Positivo_smoteT\n",
    "\n",
    "\n",
    "print(\"Rpta Positivas:\",Positivo_smoteT,\",Rpta Negativas:\", Negativo_smoteT)\n",
    "X_smoteT \n",
    "\n",
    "X_smoteT_train, X_smoteT_test, y_smoteT_train, y_smoteT_test = train_test_split(X_smoteT, y_smoteT, train_size=0.7, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cf6f7-3737-4c08-8bc8-b2ad2b9dafa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Corriendo los modelos con balanceo SMOTETomek\n",
    "models = {\n",
    "    \"                   Logistic Regression\": LogisticRegression(),\n",
    "    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"                         Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n",
    "    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n",
    "   # \"                        Neural Network\": MLPClassifier(),\n",
    "    \"                         Random Forest\": RandomForestClassifier(),\n",
    "    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"                               XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n",
    "    \"                              LightGBM\": LGBMClassifier(),\n",
    "    \"                              CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_smoteT_train, y_smoteT_train)\n",
    "    print(name + \" Entrenado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c7ef1",
   "metadata": {},
   "source": [
    "### Tras entrenar con los datos balanaceados aplicando SmoteTomek , comparamos y el que nos da una mejor métrica es XGBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2864c43-c960-4d8f-bc89-6d7fbe1e27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuento de valores positivos y negativos despues de balancear con SmoteTomek\n",
    "y_smoteT_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca2934-91c2-48f9-a08c-76115afd6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test con valores smoteT\n",
    "print(\"Model Performance\\n-----------------\")\n",
    "for name, model in models.items():\n",
    "    y_smoteT_pred = model.predict(X_smoteT_test)\n",
    "    \n",
    "    y_smoteT_pred_train = model.predict(X_smoteT_train)\n",
    "\n",
    "\n",
    "    print(\n",
    "        \"\\n\" + name + \" Accuracy: {:.3f}%\\n\\t\\t\\t\\t       F1-Score: {:.5f}\\n\\t\\t\\t              Recall: {:.5f}\\n\\t\\t\\t\\t       Precision: {:.5f}\\n\\t\\t\\t\\t       Roc_Au: {:.5f}\\n\\t\\t\\t\\t\"\\\n",
    "        .format(accuracy_score(y_smoteT_test, y_smoteT_pred) * 100, f1_score(y_smoteT_test, y_smoteT_pred),recall_score(y_smoteT_test, y_smoteT_pred),precision_score(y_smoteT_test, y_smoteT_pred),roc_auc_score(y_smoteT_test, y_smoteT_pred) ))\n",
    "    \n",
    "    print( confusion_matrix(y_smoteT_test, y_smoteT_pred))\n",
    "    \n",
    "    print(\n",
    "        \"\\n\" + name + \" Accuracy_train: {:.3f}%\\n\\t\\t\\t\\t       F1-Score_train: {:.5f}\\n\\t\\t\\t\\t        Recall_Train: {:.5f}\\n\\t\\t\\t\\t       Precision_Train: {:.5f}\\n\\t\\t\\t\\t       Roc_Au_Train: {:.5f}\\n\\t\\t\\t\\t\"\\\n",
    "        .format(accuracy_score(y_smoteT_train, y_smoteT_pred_train) * 100, f1_score(y_smoteT_train, y_smoteT_pred_train),recall_score(y_smoteT_train, y_smoteT_pred_train),precision_score(y_smoteT_train, y_smoteT_pred_train),roc_auc_score(y_smoteT_train, y_smoteT_pred_train) ))\n",
    "   \n",
    "    \n",
    "    print( confusion_matrix(y_smoteT_train, y_smoteT_pred_train))\n",
    "    \n",
    "    print(\n",
    "    \"\\n\" + name + \" Overfitting accuracy:\\n\\t\\t\\t\\t\",\n",
    "    np.abs(((((accuracy_score(y_smoteT_train, y_smoteT_pred_train))-accuracy_score(y_smoteT_test, y_smoteT_pred))/(accuracy_score(y_smoteT_test, y_smoteT_pred)) *100)))\n",
    "       )\n",
    "    \n",
    "    print(\n",
    "    \"\\n\" + name + \" Overfitting recall:\\n\\t\\t\\t\\t\",\n",
    "    np.abs(((((recall_score(y_smoteT_train, y_smoteT_pred_train))-recall_score(y_smoteT_test, y_smoteT_pred))/(recall_score(y_smoteT_test, y_smoteT_pred)) *100)))\n",
    "       )\n",
    "# Overfitting        \n",
    "    print(\n",
    "    \"\\n\" + name + \" Overfitting presicion:\\n\\t\\t\\t\\t\",\n",
    "    np.abs(((((precision_score(y_smoteT_train, y_smoteT_pred_train))-precision_score(y_smoteT_test, y_smoteT_pred))/(precision_score(y_smoteT_test, y_smoteT_pred)) *100)))\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197881d-50a6-44c2-bf4a-484889f662bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos el modelo y lo guardamos en una variable para poder utilizar Pickle.\n",
    "modelo_elegido = XGBClassifier()\n",
    "modelo_elegido.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8195f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc8a33-23b7-4e29-960e-49c61a51ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos pickle para guardar el modelo y poder utilizar en streamlit.\n",
    "modelo_pickle = 'XGBchachi.pkl'\n",
    "pickle.dump(modelo_elegido, open(modelo_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621c2112-63cb-412b-9757-b8a42ed9def4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938123fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c906b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bfa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8100f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aaf6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac63572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
